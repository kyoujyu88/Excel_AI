import os
import glob
import pickle
import json
import numpy as np
import faiss
import shutil
import tempfile
from llama_cpp import Llama 

class RAGManager:
    def __init__(self, base_dir):
        self.base_dir = base_dir
        self.knowledge_dir = os.path.join(base_dir, "knowledge")
        self.db_path = os.path.join(base_dir, "vector_db")
        
        self.config_path = os.path.join(base_dir, "config.json")
        self.model_path = ""
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, "r", encoding="utf-8") as f:
                    cfg = json.load(f)
                    last_model = cfg.get("last_model", "")
                    if last_model:
                        self.model_path = os.path.join(base_dir, "gguf", last_model)
            except: pass
        
        if not self.model_path or not os.path.exists(self.model_path):
            ggufs = glob.glob(os.path.join(base_dir, "gguf", "*.gguf"))
            if ggufs: self.model_path = ggufs[0]
            else: self.model_path = ""

        if not os.path.exists(self.knowledge_dir): os.makedirs(self.knowledge_dir)
        if not os.path.exists(self.db_path): os.makedirs(self.db_path)

        self.index = None
        self.chunks = []
        self.embed_model = None 
        
        self.load_db()

    def _load_model(self):
        if not self.model_path or not os.path.exists(self.model_path):
            return "ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚config.jsonã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

        if self.embed_model is None:
            m_name = os.path.basename(self.model_path)
            print(f"Embeddingãƒ¢ãƒ‡ãƒ«èª­è¾¼ä¸­: {m_name}")
            try:
                self.embed_model = Llama(
                    model_path=self.model_path,
                    embedding=True,
                    verbose=False,
                    n_ctx=2048,
                    n_threads=6,
                    n_gpu_layers=0
                )
            except Exception as e:
                return f"ãƒ¢ãƒ‡ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}"
        return None

    def _normalize(self, vec):
        norm = np.linalg.norm(vec)
        if norm == 0: return vec
        return vec / norm

    def build_database(self, callback=None):
        def report(msg):
            print(msg)
            if callback: callback(msg)

        err = self._load_model()
        if err: 
            report(err)
            return err

        files = glob.glob(os.path.join(self.knowledge_dir, "*.txt"))
        if not files: return "çŸ¥è­˜ãƒ•ã‚¡ã‚¤ãƒ«(.txt)ãŒã‚ã‚Šã¾ã›ã‚“"

        report(f"ã€æ¤œå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã€‘")
        for f in files: report(f" - {os.path.basename(f)}")
        report("-" * 20)

        new_chunks = []
        for file_path in files:
            try:
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    text = f.read()
                    filename = os.path.basename(file_path)
                    
                    # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºè¨­å®š
                    chunk_size = 600
                    overlap = 100
                    
                    for i in range(0, len(text), chunk_size - overlap):
                        chunk_text = text[i : i + chunk_size].strip()
                        if len(chunk_text) > 20:
                            new_chunks.append(f"ã€å‡ºå…¸:{filename}ã€‘\n{chunk_text}")
            except: pass

        if not new_chunks: return "æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ"

        embeddings = []
        report(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–é–‹å§‹ ({len(new_chunks)}ä»¶)...")
        
        for i, chunk in enumerate(new_chunks):
            try:
                vec = self.embed_model.create_embedding(chunk)
                raw_vec = vec['data'][0]['embedding']
                if isinstance(raw_vec[0], list): raw_vec = raw_vec[0]
                
                np_vec = np.array(raw_vec, dtype='float32')
                embeddings.append(self._normalize(np_vec))
                
            except Exception as e:
                report(f"Error chunk {i}: {e}")
            
            if (i+1) % 5 == 0: 
                report(f"é€²æ—: {i+1}/{len(new_chunks)} å®Œäº†")

        if not embeddings: return "ãƒ™ã‚¯ãƒˆãƒ«åŒ–å¤±æ•—"

        np_embeddings = np.array(embeddings)
        dimension = np_embeddings.shape[1]

        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(np_embeddings)
        self.chunks = new_chunks

        if not os.path.exists(self.db_path): os.makedirs(self.db_path)
        
        try:
            fd, temp_path = tempfile.mkstemp(suffix=".faiss")
            os.close(fd)
            faiss.write_index(self.index, temp_path)
            
            target_path = os.path.join(self.db_path, "index.faiss")
            if os.path.exists(target_path): os.remove(target_path)
            shutil.move(temp_path, target_path)
            
            with open(os.path.join(self.db_path, "chunks.pkl"), "wb") as f:
                pickle.dump(self.chunks, f)
        except Exception as e:
            msg = f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}"
            report(msg)
            return msg

        final_msg = f"å®Œäº†ï¼ {len(new_chunks)}ä»¶å‡¦ç†ã—ã¾ã—ãŸã€‚"
        report(final_msg)
        return final_msg

    # ----------------------------------------------------------------
    # â˜…ã“ã“ãŒå¤§æ”¹é€ ãƒã‚¤ãƒ³ãƒˆï¼ã€Œãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã€
    # ----------------------------------------------------------------
    def get_context(self, query):
        if self.index is None or not self.chunks: return "", []
        err = self._load_model()
        if err: 
            print(f"RAG Error: {err}")
            return "", []

        try:
            # 1. ã¾ãšãƒ™ã‚¯ãƒˆãƒ«ã§ã€Œæ„å‘³ãŒè¿‘ã„ã‚‚ã®ã€ã‚’æ¢ã™
            vec_res = self.embed_model.create_embedding(query)
            query_vec = vec_res['data'][0]['embedding']
            if isinstance(query_vec[0], list): query_vec = query_vec[0]
            
            np_query = np.array(query_vec, dtype='float32')
            np_query = self._normalize(np_query)
            if np_query.ndim == 1: np_query = np.expand_dims(np_query, axis=0)
            
            # å¤šã‚ã«å€™è£œã‚’å–ã£ã¦ãã‚‹ï¼ˆ50ä»¶ï¼‰
            search_k = 50
            if search_k > len(self.chunks): search_k = len(self.chunks)
            
            distances, indices = self.index.search(np_query, search_k)
            
            # 2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæ–‡å­—ï¼‰ã®ä¸€è‡´åº¦ã§ãƒœãƒ¼ãƒŠã‚¹ç‚¹ã‚’ä¸ãˆã‚‹
            # æ—¥æœ¬èªã¯ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„ã®ã§ã€æ–‡å­—é›†åˆ(Set)ã®é‡ãªã‚Šå…·åˆ(Jaccardä¿‚æ•°)ã§åˆ¤å®š
            q_chars = set(query)
            
            scored_chunks = []
            
            for i, vector_score in zip(indices[0], distances[0]):
                if i < len(self.chunks) and i >= 0:
                    chunk = self.chunks[i]
                    
                    # æ–‡å­—ã®ä¸€è‡´åº¦ã‚’è¨ˆç®— (0.0 ï½ 1.0)
                    # ä¾‹: ã€Œå¤©çš‡ã€ã¨ã„ã†æ–‡å­—ãŒchunkã«ã‚ã‚Œã°ã‚¹ã‚³ã‚¢ãŒè·³ã­ä¸ŠãŒã‚‹
                    c_chars = set(chunk)
                    intersection = len(q_chars & c_chars)
                    union = len(q_chars | c_chars)
                    keyword_score = 0.0
                    if union > 0:
                        keyword_score = intersection / union
                    
                    # â˜…æœ€çµ‚ã‚¹ã‚³ã‚¢ = ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ã‚³ã‚¢ + (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ Ã— é‡ã¿)
                    # é‡ã¿ã‚’ 0.5 ã«è¨­å®šã—ã¦ã€æ–‡å­—ä¸€è‡´ã®å½±éŸ¿åŠ›ã‚’å¼·ã‚ã¾ã™
                    final_score = vector_score + (keyword_score * 0.5)
                    
                    scored_chunks.append({
                        "chunk": chunk,
                        "score": final_score,
                        "fname": chunk.split("ã€å‡ºå…¸:")[1].split("ã€‘")[0]
                    })
            
            # 3. æœ€çµ‚ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã«ä¸¦ã¹æ›¿ãˆ
            scored_chunks.sort(key=lambda x: x["score"], reverse=True)
            
            # 4. ä¸Šä½ã‚’æ¡ç”¨ã™ã‚‹ï¼ˆåã‚Šé˜²æ­¢ä»˜ãï¼‰
            results = []
            source_files = []
            file_counts = {}
            
            print(f"\n--- æ¤œç´¢ãƒ’ãƒƒãƒˆçŠ¶æ³ (Hybrid Rank) ---")
            for item in scored_chunks:
                fname = item["fname"]
                count = file_counts.get(fname, 0)
                if count >= 3: continue # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã¯3ã¤ã¾ã§
                
                results.append(item["chunk"])
                if fname not in source_files: source_files.append(fname)
                file_counts[fname] = count + 1
                
                print(f"ãƒ»Score: {item['score']:.4f} | {fname}")
                
                if len(results) >= 6: break
            print("--------------------------------\n")

            if results:
                context_text = "\n\n".join(results)
                formatted = f"\n\n### ğŸ§  çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‚ç…§ ###\n{context_text}\n#############################\n"
                return formatted, source_files
        except Exception as e:
            print(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return "", []

    def open_folder(self): os.startfile(self.knowledge_dir)
    def load_user_file(self, path):
        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f: return f.read()
        except: return None
    def load_db(self):
        try:
            idx = os.path.join(self.db_path, "index.faiss")
            chk = os.path.join(self.db_path, "chunks.pkl")
            if os.path.exists(idx) and os.path.exists(chk):
                self.index = faiss.read_index(idx)
                with open(chk, "rb") as f: self.chunks = pickle.load(f)
                print("DBèª­è¾¼å®Œäº†")
        except: pass
