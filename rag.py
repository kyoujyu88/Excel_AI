import os
import glob
import pickle
import json
import numpy as np
import faiss
import shutil
import tempfile
from llama_cpp import Llama 

class RAGManager:
    def __init__(self, base_dir):
        self.base_dir = base_dir
        self.knowledge_dir = os.path.join(base_dir, "knowledge")
        self.db_path = os.path.join(base_dir, "vector_db")
        
        self.config_path = os.path.join(base_dir, "config.json")
        self.model_path = ""
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, "r", encoding="utf-8") as f:
                    cfg = json.load(f)
                    last_model = cfg.get("last_model", "")
                    if last_model:
                        self.model_path = os.path.join(base_dir, "gguf", last_model)
            except: pass
        
        if not self.model_path or not os.path.exists(self.model_path):
            ggufs = glob.glob(os.path.join(base_dir, "gguf", "*.gguf"))
            if ggufs: self.model_path = ggufs[0]
            else: self.model_path = ""

        if not os.path.exists(self.knowledge_dir): os.makedirs(self.knowledge_dir)
        if not os.path.exists(self.db_path): os.makedirs(self.db_path)

        self.index = None
        self.chunks = []
        self.embed_model = None 
        
        self.load_db()

    def _load_model(self):
        if not self.model_path or not os.path.exists(self.model_path):
            return "ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚config.jsonã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

        if self.embed_model is None:
            m_name = os.path.basename(self.model_path)
            print(f"Embeddingãƒ¢ãƒ‡ãƒ«èª­è¾¼ä¸­: {m_name}")
            try:
                self.embed_model = Llama(
                    model_path=self.model_path,
                    embedding=True,
                    verbose=False,
                    n_ctx=2048,
                    n_threads=6,
                    n_gpu_layers=0
                )
            except Exception as e:
                return f"ãƒ¢ãƒ‡ãƒ«èª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}"
        return None

    # â˜…è¿½åŠ æ©Ÿèƒ½ï¼šãƒ™ã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã™ã‚‹ï¼ˆé•·ã•ã‚’1ã«æƒãˆã‚‹ï¼‰
    def _normalize(self, vec):
        norm = np.linalg.norm(vec)
        if norm == 0: return vec
        return vec / norm

    def build_database(self, callback=None):
        def report(msg):
            print(msg)
            if callback: callback(msg)

        err = self._load_model()
        if err: 
            report(err)
            return err

        files = glob.glob(os.path.join(self.knowledge_dir, "*.txt"))
        if not files: return "çŸ¥è­˜ãƒ•ã‚¡ã‚¤ãƒ«(.txt)ãŒã‚ã‚Šã¾ã›ã‚“"

        report(f"ã€æ¤œå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã€‘")
        for f in files: report(f" - {os.path.basename(f)}")
        report("-" * 20)

        new_chunks = []
        for file_path in files:
            try:
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    text = f.read()
                    filename = os.path.basename(file_path)
                    
                    # â˜…å¤‰æ›´ç‚¹ï¼šãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å¤§ããã—ã¦ã€æ–‡è„ˆåˆ‡ã‚Œã‚’é˜²ã
                    chunk_size = 600   # å…ƒ300
                    overlap = 100      # å…ƒ50
                    
                    for i in range(0, len(text), chunk_size - overlap):
                        chunk_text = text[i : i + chunk_size].strip()
                        if len(chunk_text) > 20: # çŸ­ã™ãã‚‹ã‚´ãƒŸãƒ‡ãƒ¼ã‚¿ã¯æ¨ã¦ã‚‹
                            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚‚å«ã‚ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã›ã‚‹ã“ã¨ã§æ¤œç´¢ãƒ’ãƒƒãƒˆç‡ã‚’ä¸Šã’ã‚‹
                            new_chunks.append(f"ã€å‡ºå…¸:{filename}ã€‘\n{chunk_text}")
            except: pass

        if not new_chunks: return "æœ‰åŠ¹ãªãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ"

        embeddings = []
        report(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–é–‹å§‹ ({len(new_chunks)}ä»¶)...")
        
        for i, chunk in enumerate(new_chunks):
            try:
                vec = self.embed_model.create_embedding(chunk)
                raw_vec = vec['data'][0]['embedding']
                if isinstance(raw_vec[0], list): raw_vec = raw_vec[0]
                
                # â˜…å¤‰æ›´ç‚¹ï¼šãƒ™ã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                np_vec = np.array(raw_vec, dtype='float32')
                embeddings.append(self._normalize(np_vec))
                
            except Exception as e:
                report(f"Error chunk {i}: {e}")
            
            if (i+1) % 5 == 0: 
                report(f"é€²æ—: {i+1}/{len(new_chunks)} å®Œäº†")

        if not embeddings: return "ãƒ™ã‚¯ãƒˆãƒ«åŒ–å¤±æ•—"

        np_embeddings = np.array(embeddings)
        dimension = np_embeddings.shape[1]

        # â˜…å¤‰æ›´ç‚¹ï¼šIndexFlatL2ï¼ˆè·é›¢ï¼‰ã‹ã‚‰ IndexFlatIPï¼ˆå†…ç©ï¼ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰ã«å¤‰æ›´
        # æ­£è¦åŒ–ã—ãŸãƒ™ã‚¯ãƒˆãƒ«åŒå£«ã®å†…ç©ã¯ã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¨åŒã˜ã«ãªã‚Šã¾ã™ã€‚
        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(np_embeddings)
        self.chunks = new_chunks

        if not os.path.exists(self.db_path): os.makedirs(self.db_path)
        
        try:
            fd, temp_path = tempfile.mkstemp(suffix=".faiss")
            os.close(fd)
            faiss.write_index(self.index, temp_path)
            
            target_path = os.path.join(self.db_path, "index.faiss")
            if os.path.exists(target_path): os.remove(target_path)
            shutil.move(temp_path, target_path)
            
            with open(os.path.join(self.db_path, "chunks.pkl"), "wb") as f:
                pickle.dump(self.chunks, f)
        except Exception as e:
            msg = f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}"
            report(msg)
            return msg

        final_msg = f"å®Œäº†ï¼ {len(new_chunks)}ä»¶å‡¦ç†ã—ã¾ã—ãŸã€‚"
        report(final_msg)
        return final_msg

    def get_context(self, query):
        if self.index is None or not self.chunks: return "", []
        err = self._load_model()
        if err: 
            print(f"RAG Error: {err}")
            return "", []

        try:
            vec_res = self.embed_model.create_embedding(query)
            query_vec = vec_res['data'][0]['embedding']
            if isinstance(query_vec[0], list): query_vec = query_vec[0]
            
            # â˜…å¤‰æ›´ç‚¹ï¼šæ¤œç´¢ã‚¯ã‚¨ãƒªã‚‚æ­£è¦åŒ–ã™ã‚‹
            np_query = np.array(query_vec, dtype='float32')
            np_query = self._normalize(np_query)
            
            if np_query.ndim == 1: np_query = np.expand_dims(np_query, axis=0)
            
            # æ¤œç´¢ä»¶æ•°ã‚’å°‘ã—å¤šã‚ã«å–ã‚‹
            k = 10
            if k > len(self.chunks): k = len(self.chunks)
            
            distances, indices = self.index.search(np_query, k)
            
            results = []
            source_files = []
            file_counts = {}
            
            print(f"\n--- æ¤œç´¢ãƒ’ãƒƒãƒˆçŠ¶æ³ (Top {k}) ---")
            for i, score in zip(indices[0], distances[0]):
                if i < len(self.chunks) and i >= 0:
                    chunk = self.chunks[i]
                    try:
                        fname = chunk.split("ã€å‡ºå…¸:")[1].split("ã€‘")[0]
                        
                        # åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã¯3ã¤ã¾ã§ã«ã™ã‚‹ï¼ˆåã‚Šé˜²æ­¢ï¼‰
                        count = file_counts.get(fname, 0)
                        if count >= 3: continue
                        
                        results.append(chunk)
                        if fname not in source_files: source_files.append(fname)
                        file_counts[fname] = count + 1
                        
                        # ã‚¹ã‚³ã‚¢ã‚’è¡¨ç¤ºï¼ˆ1.0ã«è¿‘ã„ã»ã©ä¼¼ã¦ã„ã‚‹ï¼‰
                        print(f"ãƒ»Score: {score:.4f} | {fname}")
                        
                        if len(results) >= 6: break # æœ€çµ‚çš„ã«æ¡ç”¨ã™ã‚‹ã®ã¯6å€‹
                    except: pass
            print("--------------------------------\n")

            if results:
                context_text = "\n\n".join(results)
                formatted = f"\n\n### ğŸ§  çŸ¥è­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‚ç…§ ###\n{context_text}\n#############################\n"
                return formatted, source_files
        except Exception as e:
            print(f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        
        return "", []

    def open_folder(self): os.startfile(self.knowledge_dir)
    def load_user_file(self, path):
        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f: return f.read()
        except: return None
    def load_db(self):
        try:
            idx = os.path.join(self.db_path, "index.faiss")
            chk = os.path.join(self.db_path, "chunks.pkl")
            if os.path.exists(idx) and os.path.exists(chk):
                self.index = faiss.read_index(idx)
                with open(chk, "rb") as f: self.chunks = pickle.load(f)
                print("DBèª­è¾¼å®Œäº†")
        except: pass
